{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Clustering\n",
    "\n",
    "## $k$ Nearest-Neighbor\n",
    "### Distance Measure\n",
    "The $k$ nearest neighbor is based on the simple idea that when a sample is close to other samples, changes are that they share the same label. To know how close one sample is from another sample, there needs to be a notion of close. For this application we will use euclidian distance also known as the $L^2$ norm\n",
    "\n",
    "$$ \\left\\| x \\right\\|_2 \\overset{\\textrm{def}}= \\left( \\sum_{i=1}^n x_i^2 \\right)^{1/2}$$\n",
    "\n",
    "### Classification\n",
    "In classification we would like to assign certain samples to a class. To do so we would like to implement an algorithm that assigns the class based on the classes of the $k$ nearest neighbors. Try to do so in `.code/knn.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "import knn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# import some data to play with\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# inspect the shape of the data\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n",
    "\n",
    "# and predict\n",
    "k=10\n",
    "y_predicted = knn.predict(X_train, X_test, y_train, k)\n",
    "\n",
    "# evaluate predictions using y_predicted and y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "#### TODO: probably this exercise is to extensive, rewrite it with using libraries.\n",
    "### Dataset\n",
    "Naive Bayes classifier is often used to filter emails for spam, such as in [this article](https://www.aclweb.org/anthology/E03-1059.pdf). For this exercise we will use a dataset containing samples from email messages. This dataset is available on [openml](https://www.openml.org/d/42673) and can be read in using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dictionary Keys: \n",
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n",
      "\n",
      "Dataset Feature Names: \n",
      "['F1_text_html_found', 'F1_number_of_links', 'F1_contains_javascript']\n",
      "\n",
      "Example spam data entries: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 5. 4. 3. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Example spam targets: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Example non-spam data entries: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Example non-spam targets: \n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "spam = datasets.fetch_openml('auml_eml_1_a')\n",
    "\n",
    "# lets inspect the dataset\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Dataset Dictionary Keys: \\n{0}\\n\".format(spam.keys()))\n",
    "print(\"Dataset Feature Names: \\n{0}\\n\".format(spam['feature_names']))\n",
    "\n",
    "# print a few examples of spam and non-spam\n",
    "print(\"Example spam data entries: \\n{0}\\n\".format(spam['data'][spam['target']==0][:8,:].T))\n",
    "print(\"Example spam targets: \\n{0}\\n\".format(spam['target'][spam['target']==0][:8].T))\n",
    "print(\"Example non-spam data entries: \\n{0}\\n\".format(spam['data'][spam['target']==1][:8,:].T))\n",
    "print(\"Example non-spam targets: \\n{0}\\n\".format(spam['target'][spam['target']==1][:8].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 3 features. The first is a binary feature named 'F1_text_html_found', the second is a nominal feature named 'F1_number_of_links', and the last is again a binary feature named 'F1_contains_javascript'. Using these three features, we are going to try to predict the change of a sample being spam. To do so we use:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y_k | X=x_i) = \\frac{P(Y=y_k) P(X=x_i | Y=y_k)}{P(X=x_i)}\n",
    "\\end{align}\n",
    "\n",
    "This is an alternative way of writing:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y_k | X=x_i) = \\frac{P(Y=y_k) P(X_1=x_{i,1} \\wedge \\ldots \\wedge X_d=x_{i,d}) | Y=y_k)}{P(X_1=x_{i,1} \\wedge \\ldots \\wedge X_d=x_{i,d})}\n",
    "\\end{align}\n",
    "\n",
    "The nominator is often omitted, because it scales all classes the same so when comparing classes it does not matter to omit it. Furthermore, it simplifies the calculation. Thus we ultimately we will use:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y=y_k | X=x_i) = P(Y=y_k) P(X_1=x_{i,1} \\wedge \\ldots \\wedge X_d=x_{i,d}) | Y=y_k)\n",
    "\\end{align}\n",
    "\n",
    "To use this formula, we first need to define what the probality distribution is. For a binary this is relatively simple. For the counted feature 'F1_number_of_links', we will use a probability distribution based on a histogram with bins with a width of $10$. Implement these probabilities in `code/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "import naive_bayes\n",
    "\n",
    "import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = spam.data\n",
    "y = spam.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y)\n",
    "\n",
    "feature_types = ['binary', 'histogram', 'binary']\n",
    "\n",
    "model = naive_bayes.fit(X_train, y_train, feature_types)\n",
    "\n",
    "# inspect model\n",
    "#pprint.pprint(model)\n",
    "\n",
    "y_predict = naive_bayes.predict(X_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
