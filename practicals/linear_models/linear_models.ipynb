{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "## Regression\n",
    "When applying regression, we have a dataset $\\textbf{X}$ such that\n",
    "\n",
    "$$\\textbf{X} = \\left\\{\\textbf{x}^{(1)}, \\ldots, \\textbf{x}^{(n)}\\right\\}, \\textrm{where}\\; \\textbf{x}^{(i)}\\in\\mathbb{R}^d$$ \n",
    "\n",
    "This dataset has corresponding labels \n",
    "\n",
    "$$\\textbf{y}=\\left\\{y^{(1)},\\ldots,y^{(n)}\\right\\}, \\textrm{where}\\; y^{(i)}\\in\\mathbb{R}$$ \n",
    "\n",
    "The goal is to find a hypothesis $h$ in the set of all hypothesis $H$, such that\n",
    "\n",
    "$$ h: \\mathbb{R}^d \\to \\mathbb{R}, \\textrm{where}\\; h \\in H $$\n",
    "\n",
    "This hypothesis $h$ can be written as:\n",
    "\n",
    "$$ h(\\textbf{x}) = \\sum_{j=1}^d \\theta_j x_j + \\theta_0 x_0 \\overset{*}{=} \\theta^T\\textbf{x}, \\textrm{where}\\; x_0 = 1 $$\n",
    "\n",
    "Mind that at $\\overset{*}=$ the intercept $x_0$ is put into the vector $\\textbf{x}$.\n",
    "\n",
    "Now we would like to know how far our predictions are of from the true values. To do so we need to introduce some metrics.\n",
    "\n",
    "## Some metrics\n",
    "As a first exercise we will implement the **sum of squared errors** using simple matrix operations, do this in `./code/utils.py`\n",
    "\n",
    "These exercises are based on the [scikit-learn User Guide](https://scikit-learn.org/stable/modules/linear_model.html) and more specifically [this example](https://scikit-learn.org/stable/auto_examplesA/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py).\n",
    "\n",
    "This is the mentioned example from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7400000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('code/')\n",
    "\n",
    "import utils\n",
    "\n",
    "y1 = np.asarray([1., 1.2, 3.1, 3.7])[:,np.newaxis]\n",
    "y2 = np.asarray([0.8, 1.5, 2.6, 3.1])[:,np.newaxis]\n",
    "\n",
    "sse = utils.sum_squared_errors(y1, y2)\n",
    "print(sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sum of squared errors** (SSE) can be easily extended to the **mean squared error** (MSE) and the **root mean squared error** (RMSE). Do this in `./code/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18500000000000003\n",
      "0.4301162633521314\n"
     ]
    }
   ],
   "source": [
    "mse = utils.mean_squared_error(y1, y2)\n",
    "rmse = utils.root_mean_squared_error(y1, y2)\n",
    "print(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Linear Regression\n",
    "To fit a linear regression mode, one of the solutions could be to minimise the sum of squared errors:\n",
    "\n",
    "$$ SSE(\\theta)=\\sum_{i=1}^n\\left(y^{(i)}-\\theta^T\\textbf{x}^{(i)}\\right)^2 $$\n",
    "\n",
    "A general solution can be written as:\n",
    "\n",
    "$$ \\theta = \\left(\\textbf{X}^T \\textbf{X}\\right)^{-1} \\textbf{X}^T \\textbf{y} $$\n",
    "\n",
    "Try to find this general solution yourself using the derivative of $SSE(\\theta)$ with respect to $\\theta$ and using the product rule, the first steps are given:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial SSE(\\theta)}{\\partial \\theta} \n",
    "    &= \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n \\left(y^{(i)} - \\theta^T\\textbf{x}^{(i)}\\right)^2 \\\\\n",
    "    &\\overset{\\square}= \\frac{\\partial}{\\partial \\theta} \\left(\\textbf{y} - \\textbf{X}\\theta\\right)^2 \\\\\n",
    "    &\\overset{*}= 2 \\textbf{X}^T (\\textbf{y} - \\textbf{X}\\theta)\n",
    "\\end{align}\n",
    "\n",
    "By using the definition of inner product at $\\square$ and the product rule at $*$.\n",
    "\n",
    "Now set the derivative to zero a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the general solution to fit a linear regression model to some data. For this exercise, the diabetes dataset will be used that comes with *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model, model_selection\n",
    "\n",
    "# load the dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# And inspect the shapes\n",
    "print(diabetes_X.shape)\n",
    "print(diabetes_y.shape)\n",
    "\n",
    "# Split in train and test data (why?)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(diabetes_X, diabetes_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 442 samples and the samples have 10 parameters. Thus, we need to fit a linear model having 11 parameters, the 10 parameters in the dataset and the intercept. The next exercise will be to fit a least squared erros linear model to the diabetes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3149.4411216258404\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "import linear_models\n",
    "import utils\n",
    "\n",
    "# Fit a linear model using least squared error to the training data\n",
    "theta = linear_models.lse_fit(X_train, y_train)\n",
    "\n",
    "# Predict y\n",
    "y_predicted = linear_models.lse_predict(X_test, theta)\n",
    "\n",
    "# Get performance by evaluating y_predicted with respect to y_test\n",
    "mse = utils.mean_squared_error(y_test, y_predicted)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for additional topics\n",
    "### polynomial linear fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
